{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing with fastText\n",
    "\n",
    "Natural Language Processing (NLP) is one of the hottest areas in machine learning. Its global purpose is to understand language the way humans do. [NLP subareas](https://en.wikipedia.org/wiki/Natural_language_processing) include machine translation, text classification, speech recognition, sentiment analysis, question answering, text-to-speech, etc. \n",
    "\n",
    "As in most areas of Machine Learning, NLP accuracy has improved considerably thanks to deep learning. Just to highlight the most recent and impressive achievement, in October 2016 [Microsoft Research reached human parity in speech recognition](http://blogs.microsoft.com/next/2016/10/18/historic-achievement-microsoft-researchers-reach-human-parity-conversational-speech-recognition/). For that milestone, they used a combination of Convolutional Neural Networks and LSTM networks. \n",
    "\n",
    "However, not all machine learning is deep learning, and in this notebook I would like to highlight a great example. In the summer of 2016, two interesting NLP papers were published by Facebook Research, [Bojanowski et al., 2016](https://arxiv.org/abs/1607.04606) and [Joulin et al., 2016](https://arxiv.org/abs/1607.01759). The first one proposed a new method for word embedding and the second one a method for text classification. The authors also opensourced a C++ library with the implementation of these methods, [fastText](https://github.com/facebookresearch/fastText), that rapidly attracted a lot of interest.  \n",
    "\n",
    "The reason for this interest is that fastText obtains an accuracy in text classification almost as good as the state of the art in deep learning, but it is several orders of magnitude faster. In their paper, the authors compare the accuracy and computation time of several datasets with deep nets. As an example, in the Amazon Polarity dataset, fastText achieves an accuracy of 94.6% in 10s. In the same dataset, the crepe CNN model of [Zhang and LeCun, 2016](https://arxiv.org/abs/1509.01626) achieves 94.5% in 5 days, while the Very Deep CNN model of [Conneau et al., 2016](https://arxiv.org/abs/1606.01781) achieves 95.7% in 7h. The comparison is not even fair, because while fastText's time is computed with CPUs, the CNN models are computed using Tesla K40 GPUs. \n",
    "\n",
    "In this notebook we will discuss how to easily implement several projects using a python wrapper of fastText, [fastText.py](https://github.com/salestock/fastText.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "3.6.0 |Anaconda 4.3.0 (x86_64)| (default, Dec 23 2016, 13:19:00) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]\n"
     ]
    }
   ],
   "source": [
    "#Load all libraries\n",
    "import os,sys  \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fasttext\n",
    "try:\n",
    "    from urllib.request import urlopen     # For Python 3.0 and later\n",
    "except ImportError:\n",
    "    from urllib2 import urlopen     # Fall back to Python 2's urllib2\n",
    "try:\n",
    "    from html import unescape  # python 3.4+\n",
    "except ImportError:\n",
    "    try:\n",
    "        from html.parser import HTMLParser  # python 3.x (<3.4)\n",
    "    except ImportError:\n",
    "        from HTMLParser import HTMLParser  # python 2.x\n",
    "    unescape = HTMLParser().unescape\n",
    "from sklearn.manifold import TSNE\n",
    "%pylab inline\n",
    "\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text classification\n",
    "The first task will be to perform text classification dataset DBPedia, which can be accessed [here](https://drive.google.com/drive/folders/0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M). The dataset consists of text descriptions of 14 different classes. The training set contains 560,000 reviews and the test contains 70,000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>class_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>E. D. Abbott Ltd</td>\n",
       "      <td>Abbott of Farnham E D Abbott Limited was a Br...</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Schwan-Stabilo</td>\n",
       "      <td>Schwan-STABILO is a German maker of pens for ...</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Q-workshop</td>\n",
       "      <td>Q-workshop is a Polish company located in Poz...</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Marvell Software Solutions Israel</td>\n",
       "      <td>Marvell Software Solutions Israel known as RA...</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Bergan Mercy Medical Center</td>\n",
       "      <td>Bergan Mercy Medical Center is a hospital loc...</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                               name  \\\n",
       "0      1                   E. D. Abbott Ltd   \n",
       "1      1                     Schwan-Stabilo   \n",
       "2      1                         Q-workshop   \n",
       "3      1  Marvell Software Solutions Israel   \n",
       "4      1        Bergan Mercy Medical Center   \n",
       "\n",
       "                                         description class_name  \n",
       "0   Abbott of Farnham E D Abbott Limited was a Br...    Company  \n",
       "1   Schwan-STABILO is a German maker of pens for ...    Company  \n",
       "2   Q-workshop is a Polish company located in Poz...    Company  \n",
       "3   Marvell Software Solutions Israel known as RA...    Company  \n",
       "4   Bergan Mercy Medical Center is a hospital loc...    Company  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set dataset path\n",
    "\n",
    "data_path = '/Users/ruizhang/Documents/NLP_dataset/'\n",
    "\n",
    "#Load train set\n",
    "train_file = data_path +'dbpedia_csv/train.csv'\n",
    "df = pd.read_csv(train_file, header=None, names=['class','name','description'])\n",
    "\n",
    "#Load test set\n",
    "test_file = data_path +'dbpedia_csv/test.csv'\n",
    "df_test = pd.read_csv(test_file, header=None, names=['class','name','description'])\n",
    "\n",
    "#Mapping from class number to class name\n",
    "class_dict={\n",
    "1:'Company',\n",
    "2:'EducationalInstitution',\n",
    "3:'Artist',\n",
    "4:'Athlete',\n",
    "5:'OfficeHolder',\n",
    "6:'MeanOfTransportation',\n",
    "7:'Building',\n",
    "8:'NaturalPlace',\n",
    "9:'Village',\n",
    "10:'Animal',\n",
    "11:'Plant',\n",
    "12:'Album',\n",
    "13:'Film',\n",
    "14:'WrittenWork'\n",
    "}\n",
    "df['class_name'] = df['class'].map(class_dict)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th colspan=\"4\" halign=\"left\">1</th>\n",
       "      <th colspan=\"4\" halign=\"left\">2</th>\n",
       "      <th colspan=\"2\" halign=\"left\">3</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"2\" halign=\"left\">12</th>\n",
       "      <th colspan=\"4\" halign=\"left\">13</th>\n",
       "      <th colspan=\"4\" halign=\"left\">14</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>...</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>class_name</th>\n",
       "      <td>40000</td>\n",
       "      <td>1</td>\n",
       "      <td>Company</td>\n",
       "      <td>40000</td>\n",
       "      <td>40000</td>\n",
       "      <td>1</td>\n",
       "      <td>EducationalInstitution</td>\n",
       "      <td>40000</td>\n",
       "      <td>40000</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>Album</td>\n",
       "      <td>40000</td>\n",
       "      <td>40000</td>\n",
       "      <td>1</td>\n",
       "      <td>Film</td>\n",
       "      <td>40000</td>\n",
       "      <td>40000</td>\n",
       "      <td>1</td>\n",
       "      <td>WrittenWork</td>\n",
       "      <td>40000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>description</th>\n",
       "      <td>40000</td>\n",
       "      <td>39996</td>\n",
       "      <td>MegaPath Corporation—headquartered in Pleasan...</td>\n",
       "      <td>2</td>\n",
       "      <td>40000</td>\n",
       "      <td>39992</td>\n",
       "      <td>R.B. Govt. High School (Bengali: রামদেও বাজলা...</td>\n",
       "      <td>2</td>\n",
       "      <td>40000</td>\n",
       "      <td>40000</td>\n",
       "      <td>...</td>\n",
       "      <td>Before Smile Empty Soul became Smile Empty So...</td>\n",
       "      <td>2</td>\n",
       "      <td>40000</td>\n",
       "      <td>40000</td>\n",
       "      <td>Conspirators of Pleasure (Czech: Spiklenci sl...</td>\n",
       "      <td>1</td>\n",
       "      <td>40000</td>\n",
       "      <td>39984</td>\n",
       "      <td>Tom Clancy's Net Force Explorers or Net Force...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>40000</td>\n",
       "      <td>40000</td>\n",
       "      <td>ZURB</td>\n",
       "      <td>1</td>\n",
       "      <td>40000</td>\n",
       "      <td>40000</td>\n",
       "      <td>West High School (Akron Ohio)</td>\n",
       "      <td>1</td>\n",
       "      <td>40000</td>\n",
       "      <td>40000</td>\n",
       "      <td>...</td>\n",
       "      <td>Livity</td>\n",
       "      <td>1</td>\n",
       "      <td>40000</td>\n",
       "      <td>40000</td>\n",
       "      <td>Dirty Dancing</td>\n",
       "      <td>1</td>\n",
       "      <td>40000</td>\n",
       "      <td>40000</td>\n",
       "      <td>Star Wars Republic Commando: Triple Zero</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "class           1                                                             \\\n",
       "             count unique                                                top   \n",
       "class_name   40000      1                                            Company   \n",
       "description  40000  39996   MegaPath Corporation—headquartered in Pleasan...   \n",
       "name         40000  40000                                               ZURB   \n",
       "\n",
       "class                  2          \\\n",
       "              freq  count unique   \n",
       "class_name   40000  40000      1   \n",
       "description      2  40000  39992   \n",
       "name             1  40000  40000   \n",
       "\n",
       "class                                                                     3   \\\n",
       "                                                           top   freq  count   \n",
       "class_name                              EducationalInstitution  40000  40000   \n",
       "description   R.B. Govt. High School (Bengali: রামদেও বাজলা...      2  40000   \n",
       "name                             West High School (Akron Ohio)      1  40000   \n",
       "\n",
       "class               ...                                                   12  \\\n",
       "            unique  ...                                                  top   \n",
       "class_name       1  ...                                                Album   \n",
       "description  40000  ...     Before Smile Empty Soul became Smile Empty So...   \n",
       "name         40000  ...                                               Livity   \n",
       "\n",
       "class                  13         \\\n",
       "              freq  count unique   \n",
       "class_name   40000  40000      1   \n",
       "description      2  40000  40000   \n",
       "name             1  40000  40000   \n",
       "\n",
       "class                                                                     14  \\\n",
       "                                                           top   freq  count   \n",
       "class_name                                                Film  40000  40000   \n",
       "description   Conspirators of Pleasure (Czech: Spiklenci sl...      1  40000   \n",
       "name                                             Dirty Dancing      1  40000   \n",
       "\n",
       "class                                                                         \n",
       "            unique                                                top   freq  \n",
       "class_name       1                                        WrittenWork  40000  \n",
       "description  39984   Tom Clancy's Net Force Explorers or Net Force...     15  \n",
       "name         40000           Star Wars Republic Commando: Triple Zero      1  \n",
       "\n",
       "[3 rows x 56 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.describe().transpose()\n",
    "desc = df.groupby('class')\n",
    "desc.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to treat the data. As of today, the python wrapper of fastText doesn't allow dataframes or iterators as inputs to their functions (however, they are [working on it](https://github.com/salestock/fastText.py/issues/78)). We have to create an intermediate file. This intermediate file doesn't have commas, non-ascii characters and everything is lowercase. The changes are based on [this script](https://github.com/facebookresearch/fastText/blob/a88344f6de234bdefd003e9e55512eceedde3ec0/classification-example.sh#L17)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_dataset(dataframe, shuffle=False, encode_ascii=False, clean_strings = False, label_prefix='__label__'):\n",
    "    # Transform train file\n",
    "    df = dataframe[['name','description']].apply(lambda x: x.str.replace(',',' '))\n",
    "    df['class'] = label_prefix + dataframe['class'].astype(str) + ' '\n",
    "    if clean_strings:\n",
    "        df[['name','description']] = df[['name','description']].apply(lambda x: x.str.replace('\"',''))\n",
    "        df[['name','description']] = df[['name','description']].apply(lambda x: x.str.replace('\\'',' \\' '))\n",
    "        df[['name','description']] = df[['name','description']].apply(lambda x: x.str.replace('.',' . '))\n",
    "        df[['name','description']] = df[['name','description']].apply(lambda x: x.str.replace('(',' ( '))\n",
    "        df[['name','description']] = df[['name','description']].apply(lambda x: x.str.replace(')',' ) '))\n",
    "        df[['name','description']] = df[['name','description']].apply(lambda x: x.str.replace('!',' ! '))\n",
    "        df[['name','description']] = df[['name','description']].apply(lambda x: x.str.replace('?',' ? '))\n",
    "        df[['name','description']] = df[['name','description']].apply(lambda x: x.str.replace(':',' '))\n",
    "        df[['name','description']] = df[['name','description']].apply(lambda x: x.str.replace(';',' '))\n",
    "        df[['name','description']] = df[['name','description']].apply(lambda x: x.str.lower())\n",
    "    if shuffle:\n",
    "        df.sample(frac=1).reset_index(drop=True)\n",
    "    if encode_ascii :\n",
    "        df[['name','description']] = df[['name','description']].apply(lambda x: x.str.normalize('NFKD').str.encode('ascii','ignore').str.decode('utf-8'))\n",
    "    df['name'] = ' ' + df['name'] + ' '\n",
    "    df['description'] = ' ' + df['description'] + ' '\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.9 s, sys: 3.97 s, total: 14.8 s\n",
      "Wall time: 19.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Transform datasets\n",
    "df_train_clean = clean_dataset(df, True, False)\n",
    "df_test_clean = clean_dataset(df_test, False, False)\n",
    "\n",
    "# Write files to disk\n",
    "train_file_clean =data_path + 'dbpedia.train'\n",
    "df_train_clean.to_csv(train_file_clean, header=None, index=False, columns=['class','name','description'] )\n",
    "\n",
    "test_file_clean = data_path + 'dbpedia.test'\n",
    "df_test_clean.to_csv(test_file_clean, header=None, index=False, columns=['class','name','description'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dataset is cleaned, the next step is to train the classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 8s, sys: 12 s, total: 1min 20s\n",
      "Wall time: 42.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train a classifier\n",
    "output_file = data_path + 'dp_model'\n",
    "classifier = fasttext.supervised(train_file_clean, output_file, label_prefix='__label__')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is trained, we can test its accuracy. We can obtain the [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall) of the model. High precision means that an algorithm returned substantially more relevant results than irrelevant ones, while high recall means that an algorithm returned most of the relevant results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P@1: 0.9734571428571429\n",
      "R@1: 0.9734571428571429\n",
      "Number of examples: 70000\n",
      "CPU times: user 1.29 s, sys: 758 ms, total: 2.05 s\n",
      "Wall time: 2.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Evaluate classifier\n",
    "result = classifier.test(test_file_clean)\n",
    "print('P@1:', result.precision)\n",
    "print('R@1:', result.recall)\n",
    "print ('Number of examples:', result.nexamples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to check how the model works with real sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  Picasso was a famous painter born in Malaga, Spain. He revolutionized the art in the 20th century.\n",
      "Label: 3; label name: Artist\n",
      "Sentence:  One of my favourite tennis players in the world is Rafa Nadal.\n",
      "Label: 9; label name: Village; certainty: 0.375000\n",
      "Sentence:  Say what one more time, I dare you, I double-dare you motherfucker!\n",
      "Label: 12; label name: Album; certainty: 0.865234\n",
      "Label: 1; label name: Company; certainty: 0.044922\n",
      "Label: 14; label name: WrittenWork; certainty: 0.044922\n"
     ]
    }
   ],
   "source": [
    "sentence1 = ['Picasso was a famous painter born in Malaga, Spain. He revolutionized the art in the 20th century.']\n",
    "labels1 = classifier.predict(sentence1)\n",
    "class1 = int(labels1[0][0])\n",
    "print(\"Sentence: \", sentence1[0])\n",
    "print(\"Label: %d; label name: %s\" %(class1, class_dict[class1]))\n",
    "\n",
    "sentence2 = ['One of my favourite tennis players in the world is Rafa Nadal.']\n",
    "labels2 = classifier.predict_proba(sentence2)\n",
    "class2, prob2 = labels2[0][0] # it returns class2 as string\n",
    "print(\"Sentence: \", sentence2[0])\n",
    "print(\"Label: %s; label name: %s; certainty: %f\" %(class2, class_dict[int(class2)], prob2))\n",
    "\n",
    "sentence3 = ['Say what one more time, I dare you, I double-dare you motherfucker!']\n",
    "number_responses = 3\n",
    "labels3 = classifier.predict_proba(sentence3, k=number_responses)\n",
    "print(\"Sentence: \", sentence3[0])\n",
    "for l in range(number_responses):\n",
    "    class3, prob3 = labels3[0][l]\n",
    "    print(\"Label: %s; label name: %s; certainty: %f\" %(class3, class_dict[int(class3)], prob3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model predicts sentence 1 as `Artist`, which is correct. Sentence 2 is also predicted correctly. This time we used the function `predict_proba` that returns the certainty of the prediction as a probability. Finally, sentence 3 was not correctly classified. The correct label would be `Film`, since the sentence is from a famous scene of a very good film. If by any chance, you don't know [what I'm talking about](https://www.youtube.com/watch?v=xwT60UbOZnI), well, please put your priorities in order. Stop reading this notebook, go to see Pulp Fiction, and then come back to keep learning NLP :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Sentiment Analysis\n",
    "Sentiment analysis is one of the most important use cases in text classification. The objective is to classify a piece of text into positive, negative and, in some cases, neutral. This is extensively used by brands to understand the perception their customers have on their products. Sentiment analysis can influence marketing campaigns, generate leads, plan product development or improve customer service.  \n",
    "\n",
    "In our notebook, we will use the Amazon polarity dataset, which contains 3.6 million reviews in the train set and 400,000 reviews in the test set. The reviews are positive or negative. The dataset can be found [here](https://drive.google.com/drive/folders/0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M). \n",
    "\n",
    "The first step is to prepare the dataset for the algorithm format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Load train set\n",
    "train_file = data_path + 'amazon_review_polarity_train.csv'\n",
    "df_sentiment_train = pd.read_csv(train_file, header=None, names=['class','name','description'])\n",
    "\n",
    "#Load test set\n",
    "test_file = data_path + 'amazon_review_polarity_test.csv'\n",
    "df_sentiment_test = pd.read_csv(test_file, header=None, names=['class','name','description'])\n",
    "\n",
    "# Transform datasets\n",
    "df_train_clean = clean_dataset(df_sentiment_train, True, False)\n",
    "df_test_clean = clean_dataset(df_sentiment_test, False, False)\n",
    "\n",
    "# Write files to disk\n",
    "train_file_clean = data_path + 'amazon.train'\n",
    "df_train_clean.to_csv(train_file_clean, header=None, index=False, columns=['class','name','description'] )\n",
    "\n",
    "test_file_clean = data_path + 'amazon.test'\n",
    "df_test_clean.to_csv(test_file_clean, header=None, index=False, columns=['class','name','description'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the data, let's train the classifier. This time instead of the default parameters we are going to use those used in the [fastText paper](https://arxiv.org/abs/1607.01759)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Parameters\n",
    "dim=10\n",
    "lr=0.1\n",
    "epoch=5\n",
    "min_count=1\n",
    "word_ngrams=2\n",
    "bucket=10000000\n",
    "thread=12\n",
    "label_prefix='__label__'\n",
    "\n",
    "# Train a classifier\n",
    "output_file = data_path + 'amazon_model'\n",
    "classifier = fasttext.supervised(train_file_clean, output_file, dim=dim, lr=lr, epoch=epoch,\n",
    "                                 min_count=min_count, word_ngrams=word_ngrams, bucket=bucket,\n",
    "                                 thread=thread, label_prefix=label_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's evaluate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Evaluate classifier\n",
    "result = classifier.test(test_file_clean)\n",
    "print('P@1:', result.precision)\n",
    "print('R@1:', result.recall)\n",
    "print ('Number of examples:', result.nexamples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's evaluate some sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class_dict={\n",
    "    1:\"Negative\",\n",
    "    2:\"Positive\"\n",
    "}\n",
    "\n",
    "sentence1 = [\"The product design is nice but it's working as expected\"]\n",
    "labels1 = classifier.predict_proba(sentence1)\n",
    "class1, prob1 = labels1[0][0] # it returns class as string\n",
    "print(\"Sentence: \", sentence1[0])\n",
    "print(\"Label: %s; label name: %s; certainty: %f\" %(class1, class_dict[int(class1)], prob1))\n",
    "\n",
    "sentence2 = [\"I bought the product a month ago and it was working correctly. But now is not working great\"]\n",
    "labels2 = classifier.predict_proba(sentence2)\n",
    "class2, prob2 = labels2[0][0] # it returns class as string\n",
    "print(\"Sentence: \", sentence2[0])\n",
    "print(\"Label: %s; label name: %s; certainty: %f\" %(class2, class_dict[int(class2)], prob2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a tweet as the text input. Twitter has a [REST API](https://dev.twitter.com/rest/public) that allows you to interact with all their data. However, everything from that API requires authentication (an example can be found in this [Twitter bot](https://github.com/miguelgfierro/twitter_bot) I made). Instead, we can parse the title of the tweet url, which contains the text of the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get twitter data\n",
    "url = \"https://twitter.com/miguelgfierro/status/805827479139192832\"\n",
    "response = urlopen(url).read()\n",
    "title = str(response).split('<title>')[1].split('</title>')[0]\n",
    "print(title)\n",
    "\n",
    "# Format tweet\n",
    "tweet = unescape(title)\n",
    "print(tweet)\n",
    "\n",
    "# Classify tweet\n",
    "label_tweet = classifier.predict_proba([tweet])\n",
    "class_tweet, prob_tweet = label_tweet[0][0]\n",
    "print(\"Label: %s; label name: %s; certainty: %f\" %(class_tweet, class_dict[int(class_tweet)], prob_tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word representations\n",
    "We can also use fastText to learn word vectors. This is a way to represent words in a multidimensional space, where each word is represented as a vector. Similar words will be clustered in a specific region of this multidimensional space. \n",
    "\n",
    "As an example we are going to download the English Wikipedia 9 dataset: [enwik9](http://mattmahoney.net/dc/enwik9.zip). The data is UTF-8 encoded XML consisting primarily of English text. enwik9 contains 243,426 article titles, of which 85,560 are #REDIRECT to fix broken links, and the rest are regular articles.  \n",
    "\n",
    "We can use the script `wikifil.pl` by [Matt Mahoney](http://mattmahoney.net/dc/textdata.html) to clean the data. This script filters Wikipedia XML dumps to \"clean\" text consisting only of lowercase letters (a-z, converted from A-Z), and spaces (never consecutive).  All other characters are converted to spaces.  Only text which normally appears in the web browser is displayed, tables are removed, image captions are preserved, links are converted to normal text and finally digits are spelled out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "wiki_dataset_original = data_path + 'enwik9'\n",
    "wiki_dataset = data_path + 'text9'\n",
    "if not os.path.isfile(wiki_dataset):\n",
    "    os.system(\"perl wikifil.pl \" +  wiki_dataset_original + \" > \" + wiki_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the Skipgram model of the dataset. We will obtain a vector of 50 dimensions for each word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Learn the word representation using skipgram model\n",
    "output_skipgram = data_path + 'skipgram'\n",
    "if os.path.isfile(output_skipgram + '.bin'):\n",
    "    skipgram = fasttext.load_model(output_skipgram + '.bin')\n",
    "else:\n",
    "    skipgram = fasttext.skipgram(wiki_dataset, output_skipgram, lr=0.02, dim=50, ws=5,\n",
    "        epoch=1, min_count=5, neg=5, loss='ns', bucket=2000000, minn=3, maxn=6,\n",
    "        thread=4, t=1e-4, lr_update_rate=100)\n",
    "print(np.asarray(skipgram['king']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each word is represented in a multidimensional space, we can compute the similarity between words. In that space, the difference between king and queen will be smaller than between king and woman. Makes sense, right? At the same time, the difference between man and woman will be smaller than king and woman. \n",
    "\n",
    "With word representations, we are able to map from words to numerical values. What we are doing in fact is to featurize the words. The feautures can be used in different algorithms to learn the meaning of a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Number of words in the model: \", len(skipgram.words))\n",
    "\n",
    "# Get the vector of some word\n",
    "Droyals = np.sqrt(pow(np.asarray(skipgram['king']) - np.asarray(skipgram['queen']), 2)).sum()\n",
    "print(Droyals)\n",
    "Dpeople = np.sqrt(pow(np.asarray(skipgram['king']) - np.asarray(skipgram['woman']), 2)).sum()\n",
    "print(Dpeople)\n",
    "Dpeople2 = np.sqrt(pow(np.asarray(skipgram['man']) - np.asarray(skipgram['woman']), 2)).sum()\n",
    "print(Dpeople2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word space visualization\n",
    "\n",
    "Once we have the vectorial word representation, where each word is located in a 50 dimensional space, it would be great if we could visualize the words in a reduced space.\n",
    "\n",
    "For that we will use t-Distributed Stochastic Neighbor Embedding (t-SNE) from [van der Mateen and Hinton (2008)](http://jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf). t-SNE is a dimensionality reduction method that is particularly well suited for the visualization of high-dimensional datasets. The idea is to compute the probability distribution of pairs of high dimensional objects in such a way that similar objects have high probability of being clustered together and disimilar objects have low probability of being clustered together. Afterwards, the algorithm projects these probabilities in the low dimensional space and optimizes the distance with respect to the object's location in that space. Therefore, at the end of the optimization, similar objects are close in the low dimensional space. \n",
    "\n",
    "First, we must define the words that we are going to analyze out of the complete corpus,  that contains more than 200,000 objects. We have chosen 10 words related to people and 10 words related to animals. \n",
    "\n",
    "Once we select the words to analize, we are going to get the word vectors of these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(skipgram.words))\n",
    "targets = ['man','woman','king','queen','brother','sister','father','mother','grandfather','grandmother',\n",
    "           'cat','dog','bird','squirrel','horse','pig','dove','wolf','kitten','puppy']\n",
    "classes = [1,1,1,1,1,1,1,1,1,1,\n",
    "           2,2,2,2,2,2,2,2,2,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_target=[]\n",
    "for w in targets:\n",
    "    X_target.append(skipgram[w])\n",
    "X_target = np.asarray(X_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to select a subset of the dataset, because computing more than 200.000 objects is too expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_list = list(skipgram.words)[:10000]\n",
    "X_subset=[]\n",
    "for w in word_list:\n",
    "    X_subset.append(skipgram[w])\n",
    "X_subset = np.asarray(X_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add both datasets together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_target = np.concatenate((X_subset, X_target))\n",
    "print(X_target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute the t-SNE algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "X_tsne = TSNE(n_components=2, perplexity=40, init='pca', method='exact',\n",
    "                  random_state=0, n_iter=200, verbose=2).fit_transform(X_target)\n",
    "print(X_tsne.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we draw all the words in the same plot, it would be impossible to understand anything, so we are only going to represent the target words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_tsne_target = X_tsne[-20:,:]\n",
    "print(X_tsne_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_words(X, labels, classes=None, xlimits=None, ylimits=None):\n",
    "    fig = figure(figsize=(6, 6))\n",
    "    if xlimits is not None:\n",
    "        xlim(xlimits)\n",
    "    if ylimits is not None:\n",
    "        ylim(ylimits)\n",
    "    scatter(X[:, 0], X[:, 1], c=classes)\n",
    "    for i, txt in enumerate(labels):\n",
    "        annotate(txt, (X[i, 0], X[i, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_words(X_tsne_target, targets, classes=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous plot we can see the target words represented in a 2 dimensional space. As you can see, words related to animals are clustered together. The words related to people form a couple of clusters. We can take a zoom to the cluster to the bottom right to see what words are clustered together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_words(X_tsne_target, targets, xlimits=[0.5,0.7], ylimits=[-3.7,-3.6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "In this notebook we have shown how to classify text, perform sentiment analysis, create word representations and visualize these representations in a XY plot. fastText is an easy to use framework to work with these NLP problems. Its most remarkable feature is its computation speed, that allows to train models very quick mantaining a high level of accuracy in comparison to other methods like [deep learning](https://arxiv.org/abs/1509.01626). \n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}